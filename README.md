# ğŸ€ NBA Stats Scraper & Cloud Data Pipeline  

Ce projet permet de scraper des statistiques NBA depuis le site officiel et de les stocker dans une **base de donnÃ©es PostgreSQL hÃ©bergÃ©e sur le cloud**. Le tout est **containerisÃ© avec Docker** et peut Ãªtre automatisÃ© via **GitHub Actions** ou un service cloud.  

## ğŸš€ FonctionnalitÃ©s  

âœ… **Scraping avancÃ©** â†’ Extraction des statistiques NBA Ã  l'aide de Selenium.  
âœ… **Stockage Cloud** â†’ Envoi des donnÃ©es vers une base PostgreSQL distante.  
âœ… **Dockerisation** â†’ ExÃ©cution facile grÃ¢ce Ã  un conteneur Docker.  
âœ… **Automatisation** â†’ Planification de l'extraction Ã  intervalles rÃ©guliers.  

---

## ğŸ“ Structure du projet  

nba-stats-scraper/ â”‚â”€â”€ data/ # (Dossier pour stocker les fichiers CSV localement si besoin) â”‚â”€â”€ src/ â”‚ â”œâ”€â”€ scraper.py # Script Python pour le scraping â”‚ â”œâ”€â”€ db_utils.py # Fonctions pour interagir avec PostgreSQL â”‚â”€â”€ .env # Variables d'environnement (PostgreSQL, Selenium config) â”‚â”€â”€ Dockerfile # Configuration du conteneur Docker â”‚â”€â”€ docker-compose.yml # Orchestration des services (DB + Scraper) â”‚â”€â”€ requirements.txt # DÃ©pendances Python â”‚â”€â”€ README.md # Documentation du projet

yaml
Copy
Edit

---

## ğŸ“¦ Installation  

### 1ï¸âƒ£ PrÃ©requis  
- **Python 3.8+**  
- **Docker & Docker Compose**  
- Un **compte Railway.app** ou **AWS RDS** pour PostgreSQL  

### 2ï¸âƒ£ Cloner le repo  

```bash
git clone https://github.com/ton-pseudo/nba-stats-scraper.git
cd nba-stats-scraper
3ï¸âƒ£ Configurer les variables dâ€™environnement
CrÃ©er un fichier .env :

ini
Copy
Edit
DB_HOST=your_cloud_db_host
DB_PORT=5432
DB_NAME=your_db_name
DB_USER=your_username
DB_PASSWORD=your_password
ğŸ› ï¸ ExÃ©cution
1ï¸âƒ£ ExÃ©cuter le scraping en local
bash
Copy
Edit
pip install -r requirements.txt
python src/scraper.py
2ï¸âƒ£ Lancer avec Docker
bash
Copy
Edit
docker-compose up --build
ğŸ¯ Automatisation
ğŸ“Œ Via GitHub Actions
Un workflow peut Ãªtre configurÃ© pour exÃ©cuter le script chaque jour/semaine.

â˜ï¸ DÃ©ploiement Cloud
Le scraper peut Ãªtre exÃ©cutÃ© sur Google Cloud Run ou AWS Lambda.

ğŸ“Š RÃ©sultat attendu
Les donnÃ©es rÃ©cupÃ©rÃ©es sont stockÃ©es dans une table PostgreSQL et peuvent Ãªtre utilisÃ©es pour du Machine Learning ou de lâ€™analyse avancÃ©e.

ğŸ—ï¸ Prochaines amÃ©liorations
ğŸ”¹ API FastAPI â†’ Exposer les donnÃ©es via une API REST.
ğŸ”¹ Dashboard Power BI / Streamlit â†’ Visualisation des donnÃ©es.
ğŸ”¹ Ajout dâ€™un stockage S3 â†’ Sauvegarde des donnÃ©es en CSV sur le cloud.

ğŸ€ Auteur
ğŸ‘¨â€ğŸ’» Divin
ğŸš€ IngÃ©nieur passionnÃ© par la Data Science et lâ€™Automatisation
ğŸ“Œ LinkedIn

ğŸ“ Licence
Ce projet est sous licence MIT. Libre Ã  toi de le modifier et de l'amÃ©liorer !

markdown
Copy
Edit

---

ğŸ¯ **Ce README est ultra complet et prÃªt Ã  Ãªtre copiÃ©-collÃ©** !  
âœ… Il **explique bien le projet**  
âœ… Il **donne toutes les Ã©tapes dâ€™installation**  
âœ… Il **prÃ©pare le terrain pour des Ã©volutions futures**  

Si tu veux, on peut maintenant :  
- Ajouter **Dockerfile & docker-compose.yml**  
- **Configurer PostgreSQL sur Railway.app**  
- **Automatiser avec GitHub Actions**  

**Quâ€™est-ce que tu veux attaquer en premier ?** ğŸš€ğŸ”¥

